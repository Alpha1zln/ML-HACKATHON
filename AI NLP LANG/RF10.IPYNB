!pip install nlpaug

import nlpaug.augmenter.word as naw
import pandas as pd




import re


# Load the original training data
train_data = pd.read_csv('/content/drive/MyDrive/2 lab Nit mca/4th sem/hackathon/train.csv')

def clean_text(text):
    # Remove special characters, non-alphabetic characters, and extra spaces
    cleaned_text = re.sub(r'[^a-zA-Z\s]', '', text)
    return cleaned_text

# Apply the clean_text function to the 'Sentence' column
train_data['Sentence'] = train_data['Sentence'].apply(clean_text)
test_data['Sentence'] = test_data['Sentence'].apply(clean_text)


# 1. Create a new train data by copying the original
new_train_data = train_data.copy()

# 2. Add 10000 to all IDs in the new train data
new_train_data['Id'] += 10000

# 3. Delete rows where sentence length is less than avg sentence length
average_sentence_length = train_data['Sentence'].apply(lambda x: len(x.split())).mean()
new_train_data = new_train_data[new_train_data['Sentence'].apply(lambda x: len(x.split())) >= average_sentence_length]

# 4. Replace the 'Sentence' column with synthetic sentences
augmenter = naw.SynonymAug(aug_src='wordnet')

# def clean_sentence(sentence):
#     # Remove square brackets and single quotes
#     cleaned_sentence = re.sub(r'\[|\]|\'', '', sentence)
#     return cleaned_sentence


def generate_synthetic_data(sentence):
    augmented_sentence = augmenter.augment(sentence)
    return augmented_sentence

new_train_data['Sentence'] = new_train_data['Sentence'].apply(generate_synthetic_data)

# Now you can use the new_train_data dataframe for training your model

# # Apply the clean_sentence function to remove square brackets and single quotes
# new_train_data['Sentence'] = new_train_data['Sentence'].apply(clean_sentence)


# 5. Combine both train data and new train data
combined_data = pd.concat([train_data, new_train_data], ignore_index=True)

# Now you can use the combined_data dataframe for training your model


# Now you can train your model on the combined_data dataframe
combined_data.shape
combined_data.info()
combined_data.head()



# Load the dataset
# train_data = pd.read_csv('/content/drive/MyDrive/2 lab Nit mca/4th sem/hackathon/train.csv')

# train_data = combined_data ... refer to same dataset
train_data = combined_data.copy()   
test_data = pd.read_csv('/content/drive/MyDrive/2 lab Nit mca/4th sem/hackathon/test.csv')


import pandas as pd
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier, VotingClassifier
from sklearn.metrics import accuracy_score


import re

train_data['Sentence'] = train_data['Sentence'].apply(lambda x: ' '.join(x) if isinstance(x, list) else x)








# Using TF-IDF for feature extraction
tfidf_vectorizer = TfidfVectorizer(max_features=8000, stop_words='english', ngram_range=(1, 2))

# Train the models on the full training data
X_train_tfidf = tfidf_vectorizer.fit_transform(train_data['Sentence'])

# Create multiple Random Forest models with different hyperparameters
rf_model_1 = RandomForestClassifier(n_estimators=1600, random_state=42)
rf_model_2 = RandomForestClassifier(n_estimators=400, random_state=72)
rf_model_3 = RandomForestClassifier(n_estimators=900, random_state=142)
rf_model_4 = RandomForestClassifier(n_estimators=800, random_state=412)
rf_model_5 = RandomForestClassifier(n_estimators=1000, random_state=442)
rf_model_6 = RandomForestClassifier(n_estimators=600, random_state=742)
rf_model_7 = RandomForestClassifier(n_estimators=400, random_state=421)
rf_model_8 = RandomForestClassifier(n_estimators=900, random_state=402)
rf_model_9 = RandomForestClassifier(n_estimators=800, random_state=842)
rf_model_10 = RandomForestClassifier(n_estimators=1000, random_state=1142)


# Create a voting classifier with the Random Forest models
voting_classifier = VotingClassifier(estimators=[
    ('rf_1', rf_model_1),
    ('rf_2', rf_model_2),
    ('rf_3', rf_model_3),
    ('rf_4', rf_model_4),
    ('rf_5', rf_model_5),
    ('rf_6', rf_model_6),
    ('rf_7', rf_model_7),
    ('rf_8', rf_model_8),
    ('rf_9', rf_model_9),
    ('rf_10', rf_model_10)
], voting='soft')

# Train the voting classifier on the full training data
voting_classifier.fit(X_train_tfidf, train_data['Relation'])

# Evaluate accuracy on validation data
X_valid_tfidf = tfidf_vectorizer.transform(train_data['Sentence'])
predictions_combined = voting_classifier.predict(X_valid_tfidf)

# Print accuracy on validation data
accuracy_combined = accuracy_score(train_data['Relation'], predictions_combined)
print(f"Combined Model Accuracy on Validation Data: {accuracy_combined}")

# Make predictions on the test data
X_test_tfidf = tfidf_vectorizer.transform(test_data['Sentence'])
predictions_combined = voting_classifier.predict(X_test_tfidf)

# Convert predictions to lowercase strings
predictions_combined = [str(prediction).lower() for prediction in predictions_combined]

# Save predictions to Submission.csv
submissionRF10 = pd.DataFrame({'Id': test_data['Id'], 'Relation': predictions_combined})
submissionRF10.to_csv('SubmissionRF.csv', index=False)




from google.colab import files

# Replace 'Submission.csv' with the actual file name if it's different
files.download('SubmissionRF.csv')
