





# model 4 ver 666 - rf - lgbm - xg - svm -  without smote 
# Accuracy: 0.465625



from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder, StandardScaler
from sklearn.impute import SimpleImputer
from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, VotingClassifier
from sklearn.svm import SVC
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix
from xgboost import XGBClassifier
from lightgbm import LGBMClassifier
import pandas as pd
import numpy as np
import re
import matplotlib.pyplot as plt
import seaborn as sns

from sklearn.ensemble import AdaBoostClassifier
#  from catboost import CatBoostClassifier




# Load the training data
train_data = pd.read_csv("/content/drive/MyDrive/2 lab Nit mca/4th sem/hackathon/AI SYNTH HACKATHON NIT K/training_data.csv")

# Store unique values before encoding
original_labels = train_data['adapts_well_to_apartment_living'].unique()

# Get unique breed_group values from the training data
train_breed_groups = set(train_data['breed_group'])



# # Encode the 'breed_group' column
label_encoder = LabelEncoder()
# train_data['breed_group'] = label_encoder.fit_transform(train_data['breed_group'])


# # Initialize label encoder with handle_unknown='ignore'
# label_encoder = LabelEncoder(handle_unknown='ignore')
# # Encode the target variable
# train_data['adapts_well_to_apartment_living'] = label_encoder.fit_transform(train_data['adapts_well_to_apartment_living'])

# Impute missing values
numeric_cols = train_data.select_dtypes(include=['int64', 'float64']).drop(columns=['id']).columns
non_numeric_cols = train_data.select_dtypes(exclude=['int64', 'float64']).columns

numeric_imputer = SimpleImputer(strategy='mean')
train_data_imputed_numeric = train_data.copy()
train_data_imputed_numeric[numeric_cols] = numeric_imputer.fit_transform(train_data[numeric_cols])

non_numeric_imputer = SimpleImputer(strategy='most_frequent')
train_data_imputed_non_numeric = train_data.copy()
train_data_imputed_non_numeric[non_numeric_cols] = non_numeric_imputer.fit_transform(train_data[non_numeric_cols])

train_data = pd.concat([train_data_imputed_numeric[numeric_cols], train_data_imputed_non_numeric[non_numeric_cols]], axis=1)

# Extract numeric values from strings
def extract_numeric_value(string):
    if isinstance(string, str):
        numeric_values = re.findall(r'\d+', string)
        if numeric_values:
            numeric_values = [int(value) for value in numeric_values]
            return sum(numeric_values) / len(numeric_values)
        else:
            return None
    else:
        return None

train_data['weight'] = train_data['weight'].apply(extract_numeric_value)
train_data['lifspan'] = train_data['lifspan'].apply(extract_numeric_value)
train_data['height'] = train_data['height'].apply(extract_numeric_value)

# Encode the 'breed_group' column
train_data['breed_group'] = LabelEncoder().fit_transform(train_data['breed_group'])

# Split data into features and target variable
X = train_data.drop(columns=['adapts_well_to_apartment_living'])
y = train_data['adapts_well_to_apartment_living']

# Standardize the features
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)


# # Apply SMOTE to upsample the minority classes
# smote = SMOTE(random_state=972)
# X_resampled, y_resampled = smote.fit_resample(X_scaled, y)
# # Assign the upsampled data back to X_scaled and y
# X_scaled = X_resampled
# y = y_resampled

# Split data into training and validation sets
X_train, X_val, y_train, y_val = train_test_split(X_scaled, y, test_size=0.2, random_state=7223)

# # Apply SMOTE to upsample the minority classes
# smote = SMOTE(random_state=972)
# X_resampled, y_resampled = smote.fit_resample(X_train, y_train)

# Now X_resampled and y_resampled contain the upsampled data

# Continue with model training using the upsampled data

# Initialize Random Forest classifiers
# random_forest_classifiers = []
# for i in range(10):
#     rf_model = RandomForestClassifier(n_estimators=800, random_state=i)
#     rf_model.fit(X_train, y_train)
#     random_forest_classifiers.append(('rf{}'.format(i), rf_model))

# # Initialize LGBM and XGBoost classifiers with hyperparameter tuning
# lgbm_model = LGBMClassifier(n_estimators=4000, random_state=4042)
# xgb_model = XGBClassifier(n_estimators=8000, random_state=1542)

# # Initialize Voting Classifier
# voting_classifier = VotingClassifier(estimators=random_forest_classifiers + [('lgbm', lgbm_model), ('xgb', xgb_model)], voting='hard')
# Initialize Voting Classifier
# voting_classifier = VotingClassifier([('lgbm', lgbm_model), ('xgb', xgb_model)], voting='soft')

# Initialize Random Forest classifiers
random_forest_classifiers = []
for i in range(10):
    rf_model = RandomForestClassifier(n_estimators=2000, random_state=i)
    rf_model.fit(X_train, y_train)
    random_forest_classifiers.append(('rf{}'.format(i), rf_model))

# Initialize LGBM, XGBoost, SVM, and Gradient Boosting classifiers
lgbm_model = LGBMClassifier(n_estimators=3000, random_state=6042)
xgb_model = XGBClassifier(n_estimators=4000, random_state=42)
svm_model = SVC(kernel='linear', probability=True)
# gb_model = GradientBoostingClassifier(n_estimators=1000)

# Initialize Voting Classifier
voting_classifier = VotingClassifier(estimators=random_forest_classifiers + [('lgbm', lgbm_model), ('xgb', xgb_model), ('svm', svm_model)], voting='soft')


# Train the Voting Classifier
voting_classifier.fit(X_train, y_train)

# # Evaluate the Voting Classifier using cross-validation
# cv_scores = cross_val_score(voting_classifier, X_train, y_train, cv=5, scoring='accuracy')
# print("Cross-validation scores:", cv_scores)
# print("Mean CV accuracy:", cv_scores.mean())

# Predict on the validation set
y_pred = voting_classifier.predict(X_val)

# Calculate evaluation metrics
accuracy = accuracy_score(y_val, y_pred)
precision = precision_score(y_val, y_pred, average='weighted')
recall = recall_score(y_val, y_pred, average='weighted')
f1 = f1_score(y_val, y_pred, average='weighted')

# Print evaluation metrics
print("Accuracy:", accuracy)
print("Precision:", precision)
print("Recall:", recall)
print("F1 Score:", f1)

# Generate confusion matrix
conf_matrix = confusion_matrix(y_val, y_pred)

# Plot confusion matrix
import matplotlib.pyplot as plt
import seaborn as sns

plt.figure(figsize=(5, 3))
sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Greens', cbar=False)
plt.xlabel('Predicted Labels')
plt.ylabel('True Labels')
plt.title('Confusion Matrix')
plt.show()





################# 33333333333333333333333333333333333333333333

# Load the test data
test_data = pd.read_csv("/content/drive/MyDrive/2 lab Nit mca/4th sem/hackathon/AI SYNTH HACKATHON NIT K/test_data.csv")

# Display first few rows of the data
(test_data.head())

# Display column names and their data types
print(test_data.dtypes)


# Check for missing values
print(test_data.isnull().sum())

# Extract 'id' column for later use in the submission file
ids = test_data['id']

# Drop 'id' column from the test data as it is not a feature for prediction
test_data.drop(columns=['id'], inplace=True)
test_data.head()




# Apply the same preprocessing steps as done for the training data
# FOR COL = WEIGHT AND LIFSPAN ......
import re

# Function to extract numeric values from a string and calculate their average
def extract_numeric_value(string):
    # Check if the input is a string or bytes-like object
    if isinstance(string, str):
        # Find all numeric values in the string
        numeric_values = re.findall(r'\d+', string)
        # Convert numeric values to integers and calculate the average
        if numeric_values:
            # Convert to integers and calculate average
            numeric_values = [int(value) for value in numeric_values]
            return sum(numeric_values) / len(numeric_values)
        else:
            # If no numeric values found, return None
            return None
    else:
        # If input is not a string or bytes-like object, return None
        return None


# Extract numeric values from strings
test_data['weight'] = test_data['weight'].apply(extract_numeric_value)
test_data['lifspan'] = test_data['lifspan'].apply(extract_numeric_value)
test_data['height'] = test_data['height'].apply(extract_numeric_value)

# # Encode the 'breed_group' column
# test_data['breed_group'] = label_encoder.transform(test_data['breed_group'])
# Map unseen labels in the test data to a predefined value (e.g., 'Other')
# test_data['breed_group'] = test_data['breed_group'].apply(lambda x: x if x in train_breed_groups else 'Other')
label_encoder = LabelEncoder()
test_data['breed_group'] = label_encoder.fit_transform(test_data['breed_group'])


# Standardize the test data using the same scaler as the training data
test_data_scaled = scaler.transform(test_data)


# Make predictions on the test data for the missing column
test_predictions = voting_classifier.predict(test_data_scaled)

# Create a DataFrame for submission
submission_df = pd.DataFrame({'id': ids, 'adapts_well_to_apartment_living': test_predictions})

# Save the submission DataFrame to a CSV file
submission_df.to_csv("Submission_RF_Lgbm_svm_XG_MDL4_ver66_original_Vals.csv", index=False)

from google.colab import files

# Replace 'Submission.csv' with the actual file name if it's different
files.download('Submission_RF_Lgbm_svm_XG_MDL4_ver66_original_Vals.csv')
